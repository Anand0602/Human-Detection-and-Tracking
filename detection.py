# -*- coding: utf-8 -*-
"""Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLlpJK2-iTH_IhX8s2teUCq9RasaWyIy
"""

pip install opencv-python torch motpy

import cv2
import torch
from motpy import Detection, MultiObjectTracker

# Load the YOLOv5 model from the Ultralytics repository
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Initialize the multi-object tracker with a time step of 0.3 seconds
tracker = MultiObjectTracker(dt=0.3)

# Function to convert detections from YOLOv5 to the format used by motpy
def yolo_to_motpy(results):
    detections = []
    for *box, conf, cls in results.xyxy[0].cpu().numpy():
        if int(cls) == 0:  # Filter for class_id 0, which corresponds to 'person'
            detections.append(Detection(box=box, score=conf))  # Create a Detection object for each person detected
    return detections

# Open the video file for processing
cap = cv2.VideoCapture('/content/drive/MyDrive/Assignment-AIMonk_Labs/Video_1.mp4')

# Define the codec and create a VideoWriter object to save the output video in MP4 format
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'mp4v' codec for MP4 files
output_video_path = '/content/drive/MyDrive/Assignment-AIMonk_Labs/output_video_1.mp4'
out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))  # Set frame rate and frame size

# Process the video frame by frame
while cap.isOpened():
    ret, frame = cap.read()  # Read the next frame from the video
    if not ret:  # Break the loop if there are no more frames
        break

    # Use the YOLOv5 model to detect persons in the current frame
    results = model(frame)
    detections = yolo_to_motpy(results)  # Convert YOLO results to motpy format

    # Update the tracker with the current detections
    tracker.step(detections)

    # Draw bounding boxes and IDs for each active track in the frame
    for track in tracker.active_tracks():
        box = track.box  # Get the bounding box for the tracked object
        cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), 2)  # Draw rectangle around the tracked object
        cv2.putText(frame, str(track.id), (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)  # Put the track ID above the rectangle

    # Write the modified frame (with tracking information) to the output video
    out.write(frame)

# Release the video capture and writer resources
cap.release()
out.release()
cv2.destroyAllWindows()

# Print the path where the output video is saved
print(f"Output video saved at: {output_video_path}")